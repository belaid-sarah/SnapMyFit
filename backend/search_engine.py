# search_engine.py
import os
import torch
import faiss
import clip
from PIL import Image
import numpy as np
from pathlib import Path
import json

# âš¡ Ã‰vite les conflits OpenMP sur Windows
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
faiss.omp_set_num_threads(1)

device = "cuda" if torch.cuda.is_available() else "cpu"

# Variables globales
model = None
preprocess = None
index = None
image_paths = None
image_labels = None  # labels pour chaque image (robe, jupe...)
class_to_indices = None  # map: type -> list[int]
class_to_index = None  # map: type -> FAISS Index (par classe)
image_metadata = None  # infos ref/brand/prix par image (optionnel)

# Types de vÃªtements possibles
TYPES = ["robe", "jupe", "t-shirt", "pantalon", "short", "veste", "chemise"]

def initialize():
    global model, preprocess, index, image_paths, image_labels, class_to_indices, image_metadata, class_to_index
    import time

    if model is not None:
        return  # dÃ©jÃ  initialisÃ©

    init_start = time.time()
    print("ğŸ”„ [INIT] Initialisation de CLIP et FAISS...")
    
    # Charger CLIP
    print("ğŸ“¦ [INIT] Chargement du modÃ¨le CLIP ViT-B/32...")
    clip_start = time.time()
    model, preprocess = clip.load("ViT-B/32", device=device)
    clip_elapsed = time.time() - clip_start
    print(f"âœ… [INIT] CLIP chargÃ© en {clip_elapsed:.2f}s (device: {device})")

    # Charger les images de rÃ©fÃ©rence
    IMG_DIR = "images"
    img_dir = Path(IMG_DIR)
    image_paths = []
    # Charger toutes les images de la racine images/
    if img_dir.exists():
        print(f"ğŸ“‚ [INIT] Scan du dossier images/...")
        for f in img_dir.iterdir():
            if f.is_file() and f.suffix.lower() in {".jpg", ".jpeg", ".png", ".webp"}:
                image_paths.append(str(f))
        image_paths = sorted(set(image_paths))
        print(f"   â†’ {len(image_paths)} images trouvÃ©es")
    else:
        print(f"âš ï¸ [INIT] Dossier images/ non trouvÃ©")

    # Fichiers de cache
    INDEX_FILE = Path("embeddings/faiss_index.bin")
    PATHS_FILE = Path("metadata/image_paths.json")
    LABELS_FILE = Path("metadata/image_labels.json")
    META_FILE = Path("metadata/image_metadata.json")

    # Charger chemins des images (ordre stable) si existant
    print(f"ğŸ“„ [INIT] Chargement des mÃ©tadonnÃ©es...")
    if PATHS_FILE.exists():
        with open(PATHS_FILE, "r") as f:
            image_paths = json.load(f)
        print(f"   â†’ {len(image_paths)} chemins chargÃ©s depuis {PATHS_FILE}")
    else:
        image_paths = sorted(image_paths)
        with open(PATHS_FILE, "w") as f:
            json.dump(image_paths, f)
        print(f"   â†’ {len(image_paths)} chemins sauvegardÃ©s dans {PATHS_FILE}")

    # Charger labels depuis JSON (format: {"images\\file.jpg": "robe", ...})
    if LABELS_FILE.exists():
        print(f"   â†’ Chargement des labels depuis {LABELS_FILE}...")
        import sys
        sys.stdout.flush()
        with open(LABELS_FILE, "r", encoding="utf-8") as f:
            raw_labels = json.load(f)
        print(f"   â†’ {len(raw_labels)} labels chargÃ©s")
        sys.stdout.flush()
        # Normaliser les chemins (Windows backslash -> forward slash pour compatibilitÃ©)
        print(f"   â†’ Mapping des labels aux chemins... (13752 labels, cela peut prendre 10-30 secondes)")
        import sys
        sys.stdout.flush()
        
        # Optimisation: crÃ©er un index des noms de fichiers pour recherche rapide
        image_labels = {}
        path_by_name = {Path(p).name: p for p in image_paths}
        path_by_posix = {Path(p).as_posix(): p for p in image_paths}
        
        mapped_count = 0
        total_labels = len(raw_labels)
        for idx, (k, v) in enumerate(raw_labels.items()):
            # Afficher progression tous les 1000 labels
            if idx % 1000 == 0 and idx > 0:
                print(f"   â†’ Progression: {idx}/{total_labels} labels traitÃ©s...")
                sys.stdout.flush()
            
            # Normaliser le chemin
            normalized = str(Path(k)).replace("\\", "/")
            normalized_posix = Path(normalized).as_posix()
            file_name = Path(k).name
            
            # Chercher d'abord par chemin complet, puis par nom de fichier
            if normalized_posix in path_by_posix:
                image_labels[path_by_posix[normalized_posix]] = v
                mapped_count += 1
            elif file_name in path_by_name:
                image_labels[path_by_name[file_name]] = v
                mapped_count += 1
            elif Path(k).exists():
                image_labels[k] = v
                mapped_count += 1
        
        print(f"   â†’ {mapped_count} labels mappÃ©s aux chemins")
        sys.stdout.flush()
    else:
        print(f"   âš ï¸ Fichier {LABELS_FILE} non trouvÃ©, aucun label chargÃ©")
        image_labels = {}

    # Charger mÃ©tadonnÃ©es optionnelles
    if META_FILE.exists():
        print(f"   â†’ Chargement des mÃ©tadonnÃ©es depuis {META_FILE}...")
        with open(META_FILE, "r", encoding="utf-8") as f:
            image_metadata = json.load(f)
        print(f"   â†’ {len(image_metadata)} mÃ©tadonnÃ©es chargÃ©es")
    else:
        print(f"   âš ï¸ Fichier {META_FILE} non trouvÃ©, mÃ©tadonnÃ©es vides")
        image_metadata = {}

    # Construire ou charger l'index FAISS global
    print("ğŸ“¦ [INIT] Chargement de l'index FAISS global...")
    faiss_start = time.time()
    if INDEX_FILE.exists():
        index_size_mb = INDEX_FILE.stat().st_size / (1024 * 1024)
        print(f"   â†’ Fichier trouvÃ©: {INDEX_FILE} ({index_size_mb:.2f} MB)")
        print(f"   â†’ DÃ©but du chargement FAISS... (cela peut prendre 10-60 secondes sur HDD)")
        print(f"   â†’ Si Ã§a prend trop de temps, vÃ©rifiez votre antivirus ou le type de disque")
        
        # Afficher un message toutes les 10 secondes pour montrer que Ã§a progresse
        import sys
        sys.stdout.flush()
        
        try:
            # Charger l'index FAISS (peut Ãªtre lent sur HDD ou si antivirus scanne)
            print(f"   â†’ Lecture du fichier en cours... (patientez)")
            sys.stdout.flush()
            
            # Essayer de charger avec un indicateur de progression
            index = faiss.read_index(str(INDEX_FILE))
            faiss_elapsed = time.time() - faiss_start
            print(f"âœ… [INIT] Index global chargÃ© en {faiss_elapsed:.2f}s ({index.ntotal} vecteurs)")
            sys.stdout.flush()
        except Exception as e:
            faiss_elapsed = time.time() - faiss_start
            print(f"âŒ [INIT] Erreur lors du chargement de l'index aprÃ¨s {faiss_elapsed:.2f}s: {e}")
            print(f"   â†’ Tentative de reconstruction...")
            # Reconstruire l'index si le fichier est corrompu
            if image_paths and len(image_paths) > 0:
                print(f"   â†’ Reconstruction depuis {len(image_paths)} images (cela peut prendre plusieurs minutes)...")
                embeddings = np.vstack([get_embedding(p) for p in image_paths]).astype("float32")
                dimension = embeddings.shape[1]
                index = faiss.IndexFlatL2(dimension)
                index.add(embeddings)
                faiss.write_index(index, str(INDEX_FILE))
                print(f"âœ… [INIT] Index reconstruit et sauvegardÃ©")
            else:
                print(f"âš ï¸ [INIT] Pas d'images disponibles, index non crÃ©Ã©")
                index = None
    else:
        print(f"âš ï¸ [INIT] Index global non trouvÃ©, construction depuis les images...")
        print(f"   â†’ Cela peut prendre du temps pour {len(image_paths)} images...")
        embeddings = np.vstack([get_embedding(p) for p in image_paths]).astype("float32")
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        faiss.write_index(index, str(INDEX_FILE))
        faiss_elapsed = time.time() - faiss_start
        print(f"âœ… [INIT] Index global construit et sauvegardÃ© en {faiss_elapsed:.2f}s")

    # Construire ou charger des index FAISS par classe (si dataset organisÃ© ou labels dÃ©jÃ  partiels)
    class_to_indices = {t: [] for t in TYPES}
    for i, p in enumerate(image_paths):
        t = image_labels.get(p)
        if t in class_to_indices:
            class_to_indices[t].append(i)

    # Charger les index par classe
    print("ğŸ“¦ [INIT] Chargement des index FAISS par classe...")
    import sys
    sys.stdout.flush()
    class_start = time.time()
    class_to_index = {}
    loaded_classes = []
    for t in TYPES:
        indices = class_to_indices.get(t, [])
        if not indices:
            continue
        idx_path = Path(f"embeddings/faiss_index_{t}.bin")
        if idx_path.exists():
            idx_size_mb = idx_path.stat().st_size / (1024 * 1024)
            print(f"   â†’ Chargement de l'index '{t}' ({idx_size_mb:.2f} MB)...")
            sys.stdout.flush()
            try:
                class_to_index[t] = faiss.read_index(str(idx_path))
                loaded_classes.append(f"{t} ({len(indices)} images, {idx_size_mb:.2f} MB)")
                print(f"   âœ… Index '{t}' chargÃ©")
                sys.stdout.flush()
            except Exception as e:
                print(f"   âš ï¸ Erreur lors du chargement de l'index '{t}': {e}")
                print(f"   â†’ L'index sera reconstruit Ã  la prochaine recherche")
                sys.stdout.flush()
        else:
            # Reconstituer les vecteurs Ã  partir de l'index global
            print(f"   â†’ Construction de l'index pour '{t}' ({len(indices)} images)...")
            xb = np.vstack([index.reconstruct(i) for i in indices]).astype("float32")
            sub_index = faiss.IndexFlatL2(xb.shape[1])
            sub_index.add(xb)
            faiss.write_index(sub_index, str(idx_path))
            class_to_index[t] = sub_index
            loaded_classes.append(f"{t} ({len(indices)} images, construit)")
    
    class_elapsed = time.time() - class_start
    if loaded_classes:
        print(f"âœ… [INIT] {len(loaded_classes)} index par classe chargÃ©s en {class_elapsed:.2f}s")
        for cls_info in loaded_classes:
            print(f"   â†’ {cls_info}")
    else:
        print(f"âš ï¸ [INIT] Aucun index par classe disponible")

    total_elapsed = time.time() - init_start
    print(f"âœ… [INIT] Initialisation complÃ¨te en {total_elapsed:.2f}s")
    print(f"ğŸ“Š [INIT] Index prÃªt avec {len(image_paths)} images au total.")

def get_embedding(image_path: str) -> np.ndarray:
    global model, preprocess
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
    with torch.no_grad():
        emb = model.encode_image(image)
    return emb.cpu().numpy()

# Cache pour les text_features (ne changent jamais, calculÃ©s une seule fois)
_text_features_cache = None

def get_type_of_image(image_path: str) -> str:
    """
    Utilise CLIP pour prÃ©dire le type de vÃªtement.
    Retourne un des TYPES.
    OptimisÃ©: cache les text_features qui ne changent jamais.
    """
    global model, preprocess, _text_features_cache
    
    # Calculer text_features une seule fois et les mettre en cache
    if _text_features_cache is None:
        text_tokens = clip.tokenize(TYPES).to(device)
        with torch.no_grad():
            _text_features_cache = model.encode_text(text_tokens)
            _text_features_cache /= _text_features_cache.norm(dim=-1, keepdim=True)
    
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)

    with torch.no_grad():
        image_features = model.encode_image(image)
        image_features /= image_features.norm(dim=-1, keepdim=True)

        # SimilaritÃ© cosinus (text_features dÃ©jÃ  normalisÃ©s)
        similarity = (image_features @ _text_features_cache.T).cpu().numpy()
        idx = similarity.argmax()
        return TYPES[idx]

def search_image(query_img: str, k: int = 5):
    """
    Recherche d'images similaires. Retourne (results, predicted_type) pour Ã©viter les appels redondants.
    """
    global index, image_paths, image_labels, class_to_indices, class_to_index
    import time
    
    # VÃ©rifier que l'initialisation a Ã©tÃ© faite (normalement au dÃ©marrage)
    if index is None:
        print("âš ï¸ [SEARCH] Index non initialisÃ©, initialisation en cours...")
        initialize()  # fallback si pas initialisÃ© au dÃ©marrage

    search_start = time.time()
    
    # 1ï¸âƒ£ DÃ©tecter le type du vÃªtement uploadÃ© (une seule fois)
    print(f"\nğŸ” [SEARCH] DÃ©tection de la catÃ©gorie pour: {Path(query_img).name}")
    type_start = time.time()
    query_type = get_type_of_image(query_img)
    type_elapsed = time.time() - type_start
    print(f"âœ… [SEARCH] CatÃ©gorie dÃ©tectÃ©e: {query_type} ({type_elapsed:.2f}s)")

    # 2ï¸âƒ£ Extraire l'embedding de la requÃªte (une seule fois)
    emb_start = time.time()
    query_emb = get_embedding(query_img)
    emb_elapsed = time.time() - emb_start
    print(f"ğŸ“Š [SEARCH] Embedding extrait ({emb_elapsed:.2f}s)")

    # 3ï¸âƒ£ Filtrer candidats par type AVANT la recherche si possible
    candidate_indices = class_to_indices.get(query_type, [])
    print(f"ğŸ“Š [SEARCH] Nombre d'images dans la catÃ©gorie '{query_type}': {len(candidate_indices)}")

    # Utiliser l'index par classe si disponible (le plus rapide)
    cls_index = class_to_index.get(query_type)
    if cls_index is not None and candidate_indices:
        print(f"âš¡ [SEARCH] Recherche RAPIDE: index FAISS spÃ©cifique Ã  '{query_type}'")
        print(f"   â†’ Recherche dans {len(candidate_indices)} images au lieu de {len(image_paths)} totales")
        faiss_start = time.time()
        D, I = cls_index.search(query_emb, min(k, len(candidate_indices)))
        faiss_elapsed = time.time() - faiss_start
        selected = [image_paths[candidate_indices[i]] for i in I[0]]
        total_elapsed = time.time() - search_start
        print(f"âœ… [SEARCH] {len(selected)} rÃ©sultats trouvÃ©s en {total_elapsed:.2f}s (FAISS: {faiss_elapsed:.3f}s)")
        return selected, query_type

    # 4ï¸âƒ£ Fallback: recherche globale puis filtrage paresseux
    print(f"âš ï¸ [SEARCH] Index par classe non disponible, fallback: recherche globale")
    faiss_start = time.time()
    D, I = index.search(query_emb, min(50, len(image_paths)))  # top-50 pour limiter le coÃ»t
    faiss_elapsed = time.time() - faiss_start
    top_candidates = [image_paths[i] for i in I[0]]
    
    # Filtrer par catÃ©gorie
    updated = False
    filtered = []
    for p in top_candidates:
        lbl = image_labels.get(p)
        if not lbl:
            lbl = get_type_of_image(p)
            image_labels[p] = lbl
            updated = True
        if lbl == query_type:
            filtered.append(p)
        if len(filtered) >= k:
            break

    if updated:
        with open("metadata/image_labels.json", "w", encoding="utf-8") as f:
            json.dump(image_labels, f)

    result = filtered[:k] if filtered else top_candidates[:k]
    total_elapsed = time.time() - search_start
    print(f"âœ… [SEARCH] {len(result)} rÃ©sultats trouvÃ©s en {total_elapsed:.2f}s (FAISS: {faiss_elapsed:.3f}s)")
    return result, query_type

def get_metadata_for_image(image_path: str) -> dict:
    """Retourne des mÃ©tadonnÃ©es optionnelles pour une image (ref, brand, price, etc.)."""
    global image_metadata
    # Essayer plusieurs variantes de chemin
    path_variants = [
        image_path,
        str(Path(image_path)),
        str(Path(image_path).as_posix()),
        f"images\\{Path(image_path).name}",
        f"images/{Path(image_path).name}",
    ]
    for variant in path_variants:
        if image_metadata and variant in image_metadata:
            return image_metadata[variant]
    # fallback: gÃ©nÃ©rer une ref basÃ©e sur le nom de fichier si pas de metadata
    img_name = Path(image_path).stem
    return {"ref": f"REF-{img_name}", "name": img_name}
